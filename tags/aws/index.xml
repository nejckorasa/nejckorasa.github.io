<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AWS on Nejc Korasa</title>
    <link>https://nejckorasa.github.io/tags/aws/</link>
    <description>Recent content in AWS on Nejc Korasa</description>
    <image>
      <title>Nejc Korasa</title>
      <url>https://avatars.githubusercontent.com/nejckorasa</url>
      <link>https://avatars.githubusercontent.com/nejckorasa</link>
    </image>
    <generator>Hugo -- 0.150.1</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://nejckorasa.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka Backfill Playbook: Accessing Historical Data</title>
      <link>https://nejckorasa.github.io/posts/kafka-backfill/</link>
      <pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://nejckorasa.github.io/posts/kafka-backfill/</guid>
      <description>&lt;p&gt;Event-driven architectures with Kafka have become a standard way of building modern microservices. At first, everything works smoothly - services communicate via events, state is rebuilt from event streams, and the system scales well. But as your data grows, you face an inevitable challenge: what happens when you need to access historical events that are no longer in Kafka?&lt;/p&gt;
&lt;h2 id=&#34;1-the-problem-finite-retention--the-need-for-backfills&#34;&gt;1. The Problem: Finite Retention &amp;amp; The Need for Backfills&lt;/h2&gt;
&lt;p&gt;In a perfect world, we would keep every event log in Kafka forever. In the real world, however, storing an ever-growing history on high-performance broker disks is prohibitively expensive.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stream unzip files in S3 with Java</title>
      <link>https://nejckorasa.github.io/posts/s3-unzip/</link>
      <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://nejckorasa.github.io/posts/s3-unzip/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been spending a lot of time with AWS S3 recently building data pipelines and have encountered a surprisingly non-trivial challenge of unzipping files in an S3 bucket.
A few minutes with Google and StackOverflow made it clear many others have faced the same issue.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll explain a few options to handle the unzipping as well as the end solution which has led me to build &lt;a href=&#34;https://github.com/nejckorasa/s3-stream-unzip&#34;&gt;nejckorasa/s3-stream-unzip&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To sum up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there is no support to unzip files in S3 in-line,&lt;/li&gt;
&lt;li&gt;there also is no unzip built-in api available in AWS SDK.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to unzip you therefore need to download the files from S3, unzip and upload decompressed files back.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
