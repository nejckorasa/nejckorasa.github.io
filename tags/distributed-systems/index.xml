<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Distributed Systems on Nejc Korasa</title>
    <link>https://nejckorasa.github.io/tags/distributed-systems/</link>
    <description>Recent content in Distributed Systems on Nejc Korasa</description>
    <image>
      <title>Nejc Korasa</title>
      <url>https://avatars.githubusercontent.com/nejckorasa</url>
      <link>https://avatars.githubusercontent.com/nejckorasa</link>
    </image>
    <generator>Hugo -- 0.150.1</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://nejckorasa.github.io/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka Backfills in Practice: A Blueprint for Accessing Historical Data</title>
      <link>https://nejckorasa.github.io/posts/kafka-backfill/</link>
      <pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://nejckorasa.github.io/posts/kafka-backfill/</guid>
      <description>&lt;h1 id=&#34;kafka-backfills-in-practice-a-blueprint-for-accessing-historical-data&#34;&gt;Kafka Backfills in Practice: A Blueprint for Accessing Historical Data&lt;/h1&gt;
&lt;p&gt;Event-driven architectures with Kafka have become a standard way of building modern microservices. At first, everything works smoothly - services communicate via events, state is rebuilt from event streams, and the system scales well. But as your data grows, you face an inevitable challenge: what happens when you need to access historical events that are no longer in Kafka?&lt;/p&gt;
&lt;h2 id=&#34;1-the-problem-finite-retention--the-need-for-backfills&#34;&gt;1. The Problem: Finite Retention &amp;amp; The Need for Backfills&lt;/h2&gt;
&lt;p&gt;In a perfect world, we would keep every event log in Kafka forever. In the real world, however, storing an ever-growing history on high-performance broker disks is prohibitively expensive.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Idempotent Processing with Kafka</title>
      <link>https://nejckorasa.github.io/posts/idempotent-kafka-procesing/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://nejckorasa.github.io/posts/idempotent-kafka-procesing/</guid>
      <description>&lt;h2 id=&#34;duplicate-messages-are-inevitable&#34;&gt;Duplicate Messages are Inevitable&lt;/h2&gt;
&lt;p&gt;Duplicate messages are an inherent aspect of message-based systems and can occur for various reasons. In the context of Kafka, it is essential to ensure that your application is able to handle these duplicates effectively. As a Kafka consumer, there are several scenarios that can lead to the consumption of duplicate messages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There can be an actual duplicate message in the kafka topic you are consuming from. The consumer is reading 2 different messages that should be treated as duplicates.&lt;/li&gt;
&lt;li&gt;You consume the same message more than once due to various error scenarios that can happen, either in your application, or in the communication with a Kafka broker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To ensure the idempotent processing and handle these scenarios, it&amp;rsquo;s important to have a proper strategy to detect and handle duplicate messages.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
