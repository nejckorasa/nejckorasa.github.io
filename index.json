[{"content":"Building backend systems today will likely involve building many small, independent services that communicate and coordinate with one another to form a distributed system. While there are many resources available discussing the pros and cons of microservices, the architecture, and when it is appropriate to use, I want to focus on the functional testing of microservices and how it differs from traditional approaches.\nIn my experience, the \u0026ldquo;best testing practices\u0026rdquo; have evolved with the introduction of microservices, and traditional testing pyramids may not be the most effective or even potentially harmful in this context. In my work on various projects and companies, including the development of new digital banks and the migration of older systems to microservices as they scale, I have often encountered disagreements about the most appropriate testing strategies for microservices.\nWhy do we have tests? As software engineers, we rely on testing to verify that our code functions as expected. Testing should support refactoring, but it can sometimes make it more difficult. The purpose of testing is to define the intended behavior of the code, rather than the details of its implementation. In summary, tests should:\nConfirm that the code does what it should. Provide fast, accurate, reliable, and predictable feedback. Make maintenance easier, which is often overlooked when writing tests. Effective testing is crucial for building reliable software, and it is important to keep these goals in mind when writing tests. By focusing on the intended behavior of the code and the needs of maintenance, we can write tests that give us confidence in our code and make the development process more efficient.\nCommon Mistakes It is not uncommon to come across codebases with a large number of tests and high test coverage percentages, only to find that the code is not truly tested and that refactoring or adding new features is difficult. In my experience, this is often due to the following pitfalls:\nOverreliance on unit tests Tight coupling of tests to implementation details Avoiding these mistakes is key to writing effective tests that support the development process and ensure the reliability of the code.\nDo you need Unit Tests? One common approach to testing is the belief that all classes, functions, and methods must be tested. This can lead to a large number of unit tests and a high test coverage percentage. However, an excess of unit tests can make it difficult to change the code without also having to modify the tests. This can undermine the confidence in the code and negate the benefits of testing if the tests must be rewritten every time the code is changed.\nIn the case of microservices, which are small and independent by definition, it could be argued that the microservice itself is a unit and should be tested as an isolated component through its contracts, or in a black-box fashion. In this sense, the term \u0026ldquo;unit tests\u0026rdquo; for microservices can be thought of as implementation detail tests. Instead of focusing on unit tests, it may be more effective to consider the testing of microservices at a higher level, such as through integration tests.\nDon\u0026rsquo;t couple you tests to Implementation Details Please don\u0026#39;t couple your tests to implementation details. Tests should support refactoring, not make it harder. #SoftwareEngineering #testing https://t.co/oeHhbXD2Pc\n\u0026mdash; Nejc Korasa (@NejcKorasa) December 4, 2022 When writing tests, it is important to avoid coupling them to implementation details. This ensures that tests serve as a reliable safety net, allowing you to refactor the internals of your microservice without having to modify the tests.\nFocus on Integration Tests To avoid testing implementation details we should test from the edges of microservices, by examining the inputs and outputs of the service and verifying their correctness in an isolated manner while focusing on the interaction points and making them very explicit.\nDefine Inputs and Outputs Look at the entrypoint of the service (e.g. a REST API, Kafka consumer) to define the inputs for your tests and find the corresponding outputs (e.g. HTTP response, published Kafka message). It may be necessary to assert multiple outputs for a single input, as processing an HTTP request could result in a database update, new kafka message, and HTTP response\nTest the Microservice as an Isolated Component (Unit) Spin up the microservice and all necessary infrastructure components, such as web servers and databases, and send inputs to verify the outputs. Tools like Testcontainers for Java can help by running the application in a short-lived test mode with dependencies, such as databases and message queues, running in Docker containers.\nBy setting up specific infrastructure components in a separate test setup stage, you can isolate them from the actual tests, allowing you to change the underlying infrastructure without modifying the test methods themselves (e.g. replacing the database from PostgreSQL to NoSQL).\nThis approach is similar to hexagonal architecture, which decouples infrastructure and domain logic, but the testing strategy will differ.\nThere is a cost to it as it adds some complexity, but I have seen codebases where the benefits were worth it. Ultimately, the decision of how much complexity to add through isolation should be based on how often you anticipate changing the infrastructure of the service and whether the added complexity is justified.\nClear Definition of Microservice Behavior through Testing A test suite with a focus on integration tests will likely have fewer tests overall, but they will clearly define the expected behavior of the microservice. When examining the test suite, you should be able to get a clear understanding of what the microservice is intended to do.\nThere still is a place for Implementation Details Tests There will be parts of the code that are domain specific and only contain business logic. Those naturally isolated parts have an internal complexity of their own and this is where implementation details tests should be used. Testing all variations and edge cases will be cumbersome and too heavy to test through integration tests.\nReferences Spotify: Testing of Microservices Testcontainers for Java ","permalink":"https://nejckorasa.github.io/posts/microservice-testing/","summary":"Building backend systems today will likely involve building many small, independent services that communicate and coordinate with one another to form a distributed system. While there are many resources available discussing the pros and cons of microservices, the architecture, and when it is appropriate to use, I want to focus on the functional testing of microservices and how it differs from traditional approaches.\nIn my experience, the \u0026ldquo;best testing practices\u0026rdquo; have evolved with the introduction of microservices, and traditional testing pyramids may not be the most effective or even potentially harmful in this context.","title":"Avoid Tight Coupling of Tests to Implementation Details"},{"content":"I\u0026rsquo;ve been spending a lot of time with AWS S3 recently building data pipelines and have encountered a surprisingly non-trivial challenge of unzipping files in an S3 bucket. A few minutes with Google and StackOverflow made it clear many others have faced the same issue.\nI\u0026rsquo;ll explain a few options to handle the unzipping as well as the end solution which has led me to build nejckorasa/s3-stream-unzip.\nTo sum up:\nthere is no support to unzip files in S3 in-line, there also is no unzip built-in api available in AWS SDK. In order to unzip you therefore need to download the files from S3, unzip and upload decompressed files back.\nThis solution is simple to implement with the use of Java AWS SDK, and it probably is good enough if you are dealing with smaller files - if files are small enough you can just keep hold of decompressed files in memory and upload them back.\nAlternatively, in case of memory constraints, files can be persisted to disk storage. Great, that works.\nProblems arise with larger files. AWS Lambda, for example, has a 1024MB memory and disk space limit. A dedicated EC2 instance will solve the disk space issue, but it requires more maintenance. I\u0026rsquo;d also argue that storing 500MB+ files to disk is not the most optimal approach. That will of course depend on how many files need to be unzipped as well as the run frequency of that operation - it\u0026rsquo;s ok as a one-off but maybe not so if it needs to run daily. In any case, we really can do better.\nStreaming solution A better approach would be to stream the file from S3, download it in chunks, unzip and upload them back to S3 utilizing multipart upload. That way you completely avoid the need for disk storage and you can minimize the memory footprint by tuning the download and upload chunk sizes.\nThere are 2 parts of this solution that need to be integrated:\n1) Download and uznip Streaming S3 objects is natively supported by AWS SDK, there is a getObjectContent() method that returns the input stream containing the contents of the S3 object.\nJava provides ZipInputStream as an input stream filter for reading files in the ZIP file format. It reads ZIP content entry-by-entry and thus allows custom handling for each entry.\nStreaming object content from S3 and feeding that into ZipInputStream will give us decompressed chunks of object content we can buffer in memory.\n2) Upload unzipped chunks to S3 Uploading files to S3 is a common task and SDK supports several options to choose from, including multipart upload.\nWhat is multipart upload?\nMultipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object\u0026rsquo;s data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts.\nAfter all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.\nIn general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.\nnejckorasa/s3-stream-unzip All that is left to do now is to integrate stream download, unzip, and multipart upload. I\u0026rsquo;ve done all the hard work and built nejckorasa/s3-stream-unzip.\nJava library to manage unzipping of large files and data in AWS S3 without knowing the size beforehand and without keeping it all in memory or writing to disk.\nUnzipping is achieved without knowing the size beforehand and without keeping it all in memory or writing to disk. That makes it suitable for large data files - it has been used to unzip files of size 100GB+.\nIt supports different unzip strategies including an option to split zipped files (suitable for larger files, e.g. csv files). It\u0026rsquo;s lightweight and only requires an AmazonS3 client to run.\nIt has a simple API:\n// initialize AmazonS3 client AmazonS3 s3CLient = AmazonS3ClientBuilder.standard() // customize the client .build() // create UnzipStrategy var strategy = new NoSplitUnzipStrategy(); var strategy = new SplitTextUnzipStrategy() .withHeader(true) .withFileBytesLimit(100 * MB); // or create UnzipStrategy with additional config var config = new S3MultipartUpload.Config() .withThreadCount(5) .withQueueSize(5) .withAwaitTerminationTimeSeconds(2) .withCannedAcl(CannedAccessControlList.BucketOwnerFullControl) .withUploadPartBytesLimit(20 * MB) .withCustomizeInitiateUploadRequest(request -\u0026gt; { // customize request return request; }); var strategy = new NoSplitUnzipStrategy(config); // create S3UnzipManager var um = new S3UnzipManager(s3Client, strategy); var um = new S3UnzipManager(s3Client, strategy.withContentTypes(List.of(\u0026#34;application/zip\u0026#34;)); // unzip options um.unzipObjects(\u0026#34;bucket-name\u0026#34;, \u0026#34;input-path\u0026#34;, \u0026#34;output-path\u0026#34;); um.unzipObjectsKeyMatching(\u0026#34;bucket-name\u0026#34;, \u0026#34;input-path\u0026#34;, \u0026#34;output-path\u0026#34;, \u0026#34;.*\\\\.zip\u0026#34;); um.unzipObjectsKeyContaining(\u0026#34;bucket-name\u0026#34;, \u0026#34;input-path\u0026#34;, \u0026#34;output-path\u0026#34;, \u0026#34;-part-of-object-\u0026#34;); um.unzipObject(s3Object, \u0026#34;output-path\u0026#34;); Library is available on Maven Central and on Github.\n","permalink":"https://nejckorasa.github.io/posts/s3-unzip/","summary":"I\u0026rsquo;ve been spending a lot of time with AWS S3 recently building data pipelines and have encountered a surprisingly non-trivial challenge of unzipping files in an S3 bucket. A few minutes with Google and StackOverflow made it clear many others have faced the same issue.\nI\u0026rsquo;ll explain a few options to handle the unzipping as well as the end solution which has led me to build nejckorasa/s3-stream-unzip.\nTo sum up:","title":"Stream unzip files in S3 with Java"}]